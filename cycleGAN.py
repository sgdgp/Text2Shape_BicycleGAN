import torch
import torch.nn as nn
import itertools
from t2s_generator import *
from s2t_generator import *
from t2s_discriminator import *
from s2t_discriminator import *
from losses import *

class T2S_CycleGAN(nn.Module):
    def __init__(self, config, data):
        super(T2S_CycleGAN, self).__init__()
        self.config = config
        self.device = config.device
        self.t2s_g = T2S_G(1, 300, 256, data, self.config.num_vocab).to(self.config.device)
        self.s2t_g = S2T_G(config=config).to(self.config.device)
        self.t2s_d = T2S_D().to(self.config.device)
        self.s2t_d = S2T_D().to(self.config.device)

        self.criterionGAN = GANLoss().to(self.device)
        self.criterionL1 = torch.nn.L1Loss().to(self.device)

        self.optimizer_g = torch.optim.Adam(itertools.chain(self.t2s_g.parameters(), self.s2t_g.parameters()), lr=config.lr)
        self.optimizer_d = torch.optim.Adam(itertools.chain(self.t2s_d.parameters(), self.s2t_d.parameters()), lr=config.lr)



    def forward(self, mode="train"):
        self.fake_s = self.t2s_g(self.real_t)  
        # print("fake_s shape : ", self.fake_s.size())
        vocab_embedding = self.t2s_g.get_embedding_weight_as_tensor().detach()
        self.rec_t = self.s2t_g(self.fake_s, vocab_embedding) 
        # print("rec_t shape : ", self.rec_t.size())
        self.fake_s = self.t2s_g(self.real_t)
    
    def t2s_pred(self):
        self.fake_s = self.t2s_g(self.real_t)  
            

    def backward_D_basic(self, netD, real, fake):
        """Calculate GAN loss for the discriminator
        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator
        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """
        vocab_embedding = self.t2s_g.get_embedding_weight_as_tensor().detach()

        # Real
        pred_real = netD(real) if isinstance(netD, T2S_D) else netD(real,vocab_embedding)
        loss_D_real = self.criterionGAN(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())  if isinstance(netD, T2S_D) else netD(fake.detach(),vocab_embedding)
        loss_D_fake = self.criterionGAN(pred_fake, False)
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        return loss_D

    def backward_d_t2s(self):
        """Calculate GAN loss for discriminator D_A"""
        # fake_B = self.fake_B_pool.query(self.fake_B)
        self.loss_d_t2s = self.backward_D_basic(self.t2s_d, self.real_s, self.fake_s)

    def backward_d_s2t(self):
        """Calculate GAN loss for discriminator D_B"""
        # detaching one hot conversion history as well
        self.loss_d_s2t = self.backward_D_basic(self.s2t_d, self.real_t, self.rec_t)

    def backward_g(self):
        vocab_embedding = self.t2s_g.get_embedding_weight_as_tensor().detach()

        # GAN loss D_A(G_A(A))
        self.loss_g_t2s = self.criterionGAN(self.t2s_d(self.fake_s), True)
        # GAN loss D_B(G_B(B))
        self.loss_g_s2t = self.criterionGAN(self.s2t_d(self.rec_t,vocab_embedding), True)
        # Forward cycle loss || G_B(G_A(A)) - A||
        # self.loss_cycle = self.criterionCycle(self.rec_t, self.real_t) * lambda_cycle
        
        # combined loss and calculate gradients
        self.loss_g = self.loss_g_t2s + self.loss_g_s2t #+ self.loss_cycle
        self.loss_g.backward()
    
    def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad

    def optimize_parameters(self, update_only_d):
        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        # forward
        self.forward()      
        # print("Forward done")
        # self.loss_g_t2s = 0
        # self.loss_g_s2t = 0
        # self.loss_d_t2s = 0
        # self.loss_d_s2t = 0
        loss_list = []
        if not update_only_d:
            self.set_requires_grad([self.t2s_d, self.s2t_d], False)  # Ds require no gradients when optimizing Gs
            self.optimizer_g.zero_grad()  # set G_A and G_B's gradients to zero
            self.backward_g()             # calculate gradients for G_A and G_B
            self.optimizer_g.step()       # update G_A and G_B's weights
            # print("G backward done")
            loss_list.append(self.loss_g_t2s.item())
            loss_list.append(self.loss_g_s2t.item())

        # D_A and D_B
        self.set_requires_grad([self.t2s_d, self.s2t_d], True)
        self.optimizer_d.zero_grad()   # set D_A and D_B's gradients to zero
        self.backward_d_s2t()      # calculate gradients for D_A
        self.backward_d_t2s()      # calculate graidents for D_B
        self.optimizer_d.step()
        # print("D backward done")
        loss_list.append(self.loss_d_t2s.item())
        loss_list.append(self.loss_d_s2t.item())

        return loss_list

